"""The Olive XML OCR importer."""

import codecs
import copy
import logging
import os
import re
import sys
import zipfile
from collections import deque
from operator import itemgetter

from bs4 import BeautifulSoup
from impresso_commons.path.path_fs import canonical_path

from text_importer.helpers import (convert_page_coordinates, get_image_info,
                                   get_issue_schema, get_page_schema,
                                   keep_title, normalize_language,
                                   serialize_issue, serialize_page)
from text_importer.tokenization import insert_whitespace

logger = logging.getLogger(__name__)


def olive_toc_parser(toc_path, issue_dir, encoding="windows-1252"):
    """Parse the TOC.xml file (Olive format)."""
    with codecs.open(toc_path, 'r', encoding) as f:
        text = f.read()

    toc_data = {}

    global_counter = 0

    for page in BeautifulSoup(text, 'lxml').find_all('page'):
        page_data = {}

        for n, entity in enumerate(page.find_all("entity")):

            global_counter += 1
            item_legacy_id = entity.get("id")

            item = {
                "legacy_id": item_legacy_id,
                "id": canonical_path(
                    issue_dir,
                    name=f"i{str(global_counter).zfill(4)}",
                    extension=""
                ),
                "type": entity.get("entity_type"),
                "seq": n + 1
            }

            page_data[item_legacy_id] = item

        toc_data[int(page.get('page_no'))] = page_data

    # gather the IDs of all content items int the issue
    ids = [
        toc_data[page][item]["id"]
        for page in toc_data
        for item in toc_data[page]
    ]

    # check that these IDs are unique within the issue
    assert len(ids) == len(list(set(ids)))

    return toc_data


def normalize_hyphenation(line):
    """Normalize end-of-line hyphenated words.

    :param line: a line of OCR in JSON format
    :type line: dict (keys: coords, tokens)
    :rtype: dict (keys: coords, tokens)
    """
    for i, token in enumerate(line["t"]):
        if i == (len(line["t"]) - 1):
            if token["tx"][-1] == "-":
                token["hy"] = True
            if token["tx"] == "-" and "nf" in token:
                prev_token = line["t"][i - 1]
                line["t"] = line["t"][:-2]
                merged_token = {
                    "tx": "".join([prev_token["tx"], token["tx"]]),
                    "c": prev_token["c"][:2] + token["c"][2:],
                    "s": token["s"],
                    "hy": token["hy"]
                }
                logger.debug(
                    "Merged {} and {} => {}".format(
                        prev_token,
                        token,
                        merged_token
                    )
                )
                line["t"].append(merged_token)
    return line


def merge_tokens(tokens, line):
    merged_token = {
        "tx": "".join(
            [
                token["tx"]
                for token in tokens
            ]
        ),
        "c": tokens[0]["c"][:2] + tokens[-1]["c"][2:],
        "s": tokens[0]["s"]
    }
    logger.debug(
        "(In-line pseudo tokens) Merged {} => {} in line \"{}\"".format(
            "".join([t["tx"] for t in tokens]),
            merged_token["tx"],
            line
        )
    )
    return merged_token


def merge_pseudo_tokens(line):
    """Remove pseudo tokens from a line.

    :param line: a line of OCR in JSON format
    :type line: dict (keys: coords, tokens)
    :rtype: dict (keys: coords, tokens)
    """
    original_line = " ".join([t["tx"] for t in line["t"]])
    qids = set([
        token["qid"]
        for token in line["t"]
        if "qid" in token
    ])

    inline_qids = []

    for qid in qids:
        tokens = [
            (i, token)
            for i, token in enumerate(line["t"])
            if "qid" in token and token["qid"] == qid
        ]
        if len(tokens) > 1:
            inline_qids.append(qid)

    if len(inline_qids) == 0:
        return line

    for qid in inline_qids:
        # identify tokens to merge
        tokens = [
            (i, token)
            for i, token in enumerate(line["t"])
            if "qid" in token and token["qid"] == qid
        ]

        # remove tokens to merge from the line
        tokens_to_merge = [
            line["t"].pop(
                line["t"].index(token)
            )
            for i, token in tokens
        ]

        if len(tokens_to_merge) >= 2:
            insertion_point = tokens[0][0]
            merged_token = merge_tokens(tokens_to_merge, original_line)
            line["t"].insert(insertion_point, merged_token)

    return line


def normalize_line(line, lang):
    """Apply normalization to a line of OCR.

    :param line: a line of OCR text
    :type line: dict (keys: coords, tokens)
    :rtype: dict (keys: coords, tokens)
    """
    mw_tokens = [
        token
        for token in line["t"]
        if "qid" in token
    ]
    # apply normalization only to those lines that contain at least one
    # multi-word token (denoted by presence of `qid` field)
    if len(mw_tokens) > 0:
        line = merge_pseudo_tokens(line)
        line = normalize_hyphenation(line)

    for i, token in enumerate(line["t"]):
        if "qid" not in token and "nf" in token:
            del token["nf"]

        if "qid" in token:
            del token["qid"]

        if i == 0 and i != len(line["t"]) - 1:
            insert_ws = insert_whitespace(
                token["tx"],
                line["t"][i+1]["tx"],
                None,
                lang
            )

        elif i == 0 and i == len(line["t"]) - 1:
            insert_ws = insert_whitespace(
                token["tx"],
                None,
                None,
                lang
            )

        elif i == len(line["t"]) - 1:
            insert_ws = insert_whitespace(
                token["tx"],
                None,
                line["t"][i-1]["tx"],
                lang
            )

        else:
            insert_ws = insert_whitespace(
                token["tx"],
                line["t"][i+1]["tx"],
                line["t"][i-1]["tx"],
                lang
            )
        if not insert_ws:
            token["gn"] = True

    return line


def olive_parser(text):
    u"""Parse an Olive XML file (e.g. from Le Temps corpus).

    The main logic implemented here was derived from
    <https://github.com/dhlab-epfl/LeTemps-preprocessing/>. Each XML file
    corresponds to one article, as detected by Olive.

    :param text: a string with the textual xml file to parse
    :type text: string
    :rtype: a dictionary, with keys "meta", Â°stats", "legacy"
    """
    soup = BeautifulSoup(text, "lxml")
    root = soup.find("xmd-entity")
    page_no = root['page_no']
    identifier = root['id']
    language = root['language']
    title = soup.meta['name']
    entity_type = root['entity_type']
    issue_date = soup.meta['issue_date']

    out = {
        "meta": {
            "language": None,
            "type": {}
        },
        "r": [],
        "stats": {},
        "legacy": {"continuation_from": None, "continuation_to": None},
    }
    out["meta"]["title"] = title
    out["meta"]["page_no"] = [int(page_no)]
    out["meta"]["language"] = normalize_language(language)
    out["meta"]["type"]["raw"] = entity_type
    out["meta"]["issue_date"] = issue_date

    new_region = {
        "c": [],
        "p": []
    }

    new_paragraph = {
        "l": []
    }

    new_line = {
        "c": [],
        "t": []
    }

    new_token = {
        "c": [],
        "tx": ""
    }

    for primitive in soup.find_all("primitive"):

        # store coordinate of text areas (boxes) by page
        # 1) page number, 2) coordinate list
        region = copy.deepcopy(new_region)
        region["c"] = [int(i) for i in primitive.get('box').split(" ")]

        para = None
        line = None
        line_counter = 0

        for tag in primitive.find_all(recursive=False):

            if tag.name == "l":

                if para is None and line is None:
                    para = copy.deepcopy(new_paragraph)
                    line = copy.deepcopy(new_line)

                if line_counter > 0 and line is not None:
                    line = normalize_line(line, out["meta"]["language"])
                    para["l"].append(line)

                if tag.get("p") in ["S", "SA"] and line_counter > 0:
                    region["p"].append(para)
                    para = copy.deepcopy(new_paragraph)

                line = copy.deepcopy(new_line)
                line["c"] = [
                    int(i)
                    for i in tag.get('box').split(" ")
                ]
                line_counter += 1

            if tag.name in ["w", "q"]:

                # store coordinates of each token
                # 1) token, 2) page number, 3) coordinate list
                t = copy.deepcopy(new_token)
                t["c"] = [int(i) for i in tag.get('box').split(" ")]
                t["tx"] = tag.string
                t["s"] = int(tag.get('style_ref'))

                if tag.name == "q" and tag.get('qid') is not None:
                    qid = tag.get('qid')
                    normalized_form = soup.find('qw', qid=qid).text
                    t["nf"] = normalized_form
                    t["qid"] = qid

                # append the token to the line
                line["t"].append(t)

        # append orphan lines
        if line is not None:
            line = normalize_line(line, out["meta"]["language"])
            para["l"].append(line)

        region["p"].append(para)

        if para is not None:
            out["r"].append(region)

    out["legacy"]["id"] = identifier
    out["legacy"]["source"] = soup.link['source']
    """
    # I suspect this could be deleted
    out["legacy"]["word_count"] = int(soup.meta['wordcnt'])
    out["legacy"]["chars_count"] = int(soup.meta['total_chars_count'])
    suspicious_chars_count = int(soup.meta['suspicious_chars_count'])
    out["legacy"]["suspicious_chars_count"] = int(suspicious_chars_count)
    """
    out["legacy"]["first_id"] = soup.link['first_id']
    out["legacy"]["last_id"] = soup.link['last_id']
    out["legacy"]["next_id"] = soup.link['next_id']
    out["legacy"]["prev_id"] = soup.link['prev_id']

    if root.has_attr('continuation_from'):
        out["legacy"]["continuation_from"] = root['continuation_from']

    if root.has_attr('continuation_to'):
        out["legacy"]["continuation_to"] = root['continuation_to']

    return out


def combine_article_parts(article_parts):
    """TODO.

    :param article_parts: one or more article parts
    :type article_parts: list of dict
    :rtype: a dictionary, with keys "meta", "fulltext", "stats", "legacy"
    """
    if len(article_parts) > 1:
        # if an article has >1 part, retain the metadata
        # from the first item in the list
        article_dict = {
            "meta": {},
            "fulltext": "",
            "stats": {},
            "legacy": {}
        }
        article_dict["legacy"]["id"] = [
            ar["legacy"]["id"]
            for ar in article_parts
        ]
        article_dict["legacy"]["source"] = [
            ar["legacy"]["source"]
            for ar in article_parts
        ]
        article_dict["meta"]["type"] = {}
        article_dict["meta"]["type"]["raw"] =\
            article_parts[0]["meta"]["type"]["raw"]

        article_dict["meta"]["title"] = article_parts[0]["meta"]["title"]
        article_dict["meta"]["page_no"] = [
            int(n)
            for ar in article_parts
            for n in ar["meta"]["page_no"]
        ]

        # TODO: remove from production
        if len(article_dict["meta"]["page_no"]) > 1:
            # pdb.set_trace()
            pass

        article_dict["meta"]["language"] = {}
        article_dict["meta"]["language"] =\
            article_parts[0]["meta"]["language"]
        article_dict["meta"]["issue_date"] =\
            article_parts[0]["meta"]["issue_date"]
    elif len(article_parts) == 1:
        article_dict = next(iter(article_parts))
    else:
        article_dict = None
    return article_dict


def parse_styles(text):
    """Turn Olive style file into a dictionary.

    :param text: textual content of file `styleGallery.txt`
    :type text: str
    :rtype: list of dicts
    """
    styles = []
    regex = r'(\d{3})=(".*?"),(\d+\.?\d+),(\(.*?\))'

    for line in text.split("\r\n"):

        if line == "":
            continue

        n, font, font_size, color = re.match(regex, line).groups()
        styles.append(
            {
                "id": int(n),
                "f": font.replace('"', ""),
                "fs": float(font_size),
                "rgb": [
                    int(i)
                    for i in color.replace("(", "").replace(")", "").split(",")
                ]
            }
        )

    return styles


def recompose_page(page_number, info_from_toc, page_elements):
    """Create a page document starting from a list of page documents.

    :param page_number: page number
    :type page_number: int
    :param info_from_toc: a dictionary with page element IDs (articles, ads.)
        as keys, and dictionaries as values
    :type info_from_toc:
    :param page_elements: articles or advertisements
    :type page_elements: list of dict

    It's here that `n` attributes are assigned to each region/para/line/token.
    """
    page = {
        "r": []
    }
    ordered_elements = sorted(
        list(info_from_toc.values()), key=itemgetter('seq')
    )

    # put together the regions while keeping the order in the page
    for el in ordered_elements:

        # filter out the ids keeping only Ads or Articles
        # but escluding various other files in the archive
        if ("Ar" not in el["legacy_id"] or "Ar" not in el["legacy_id"]):
            continue

        element = page_elements[el["legacy_id"]]

        for i, region in enumerate(element["r"]):
            region["pOf"] = el["id"]

        page["r"] += element["r"]

    return page


def recompose_ToC(toc_data, articles):
    """TODO."""
    # concate content items from all pages into a single flat list
    content_items = [
        toc_data[pn][elid]
        for pn in toc_data.keys() for elid in toc_data[pn].keys()
    ]

    # filter out those items that are part of a multipart article
    contents = []
    sorted_content_items = sorted(content_items, key=itemgetter('seq'))
    for item in sorted_content_items:
        if(item["type"] == "Article" or item["type"] == "Ad"):

            # find the corresponding item in `articles`
            # by using `legacy_id` as the search key
            # if not found (raises exception) means that it's one of the
            # multipart articles, and it's ok to skip it
            legacy_id = item['legacy_id']
            article = None
            for ar in articles:
                if isinstance(ar["legacy"]["id"], list):
                    if ar["legacy"]["id"][0] == legacy_id:
                        article = ar
                else:
                    if ar["legacy"]["id"] == legacy_id:
                        article = ar

            try:
                assert article is not None
            except Exception:
                continue

            item['m'] = {}
            item['m']["id"] = item["id"]
            item['m']['pp'] = article["meta"]["page_no"]
            item['m']['l'] = article["meta"]["language"]
            item['m']['tp'] = article["meta"]["type"]["raw"].lower()

            if keep_title(article["meta"]["title"]):
                item['m']['t'] = article["meta"]["title"]

            item["l"] = {}
            item["l"]["id"] = article["legacy"]["id"]
            item["l"]["source"] = article["legacy"]["source"]

            # delete redundant fields
            del item['seq']
            del item['legacy_id']
            del item['type']
            del item['id']

            contents.append(item)
    return contents


def check_consistency(issue_obj, element_ids):
    """Perform a consistency check of Issue against the Olive files.

    Ensure that all article ids from the Olive package are found in the Issue
    object. Separate article components may have been merged into a single
    ToC item, so it's necessary to compare the set of legacy IDs with the
    set of all element IDs found within the Olive files.
    """
    # check that the IDs from the ToC.XML are found also in the issue.json
    all_item_ids = []
    for item in issue_obj.i:
        if isinstance(item["l"]["id"], list):
            all_item_ids += item["l"]["id"]
        else:
            all_item_ids.append(item["l"]["id"])

    difference_btw_sets = set(element_ids).difference(set(all_item_ids))

    try:
        assert len(difference_btw_sets) == 0
        logger.info("{} consistency check ok".format(issue_obj.id))
    except AssertionError as e:
        logger.warning(
            "There are missing elements in the issue.json: {}".format(
                difference_btw_sets
            )
        )
        raise e


def olive_import_issue(
    issue_dir,
    image_dir,
    out_dir=None,
    s3_bucket=None,
    temp_dir=None
):
    """Import newspaper issues from a directory structure.

    This function imports a set of newspaper issues from a directory
    structure containing a `Document.zip` file for each issue (in the case of
    Le Temps one issue per day).

    Each ZIP archive is read, and the XML files it contains are transformed
    into the Impresso canonical format, described by a JSON schema. Each
    resulting JSON article is validate against this schema and written to
    file, following an agreed-upon directory structure.

    The intermediate Olive XML files can be kept on request (useful for
    debugging) by passing a `temp_dir` parameter.

    :param issue_dir: an object representing a newspaper issue
    :type issue_dir: `IssueDir`

    :param out_dir: the output directory, where the resulting JSON files
        will be written.
    :type out_dir: string

    :param temp_dir: a directory where to write the intermediate Olive XML
        files (default is `None`); if  `None` these files are not kept.
    :type temp_dir: string

    :rtype: a tuple where [0] the `issue_dir`; [1] a boolean indicating whether
        the import was successful; [2] the exception message if [1] is `False`,
        otherwise `None`.
    """
    logger.info("Started importing '{}'...'".format(issue_dir.path))

    if out_dir is not None:

        out_dir = os.path.join(
            out_dir,
            canonical_path(issue_dir, path_type="dir")
        )

        if not os.path.exists(out_dir):
            os.makedirs(out_dir)

    if temp_dir is not None:
        temp_dir = os.path.join(
            temp_dir,
            canonical_path(issue_dir, path_type="dir")
        )

    if temp_dir is not None and not os.path.exists(temp_dir):
        os.makedirs(temp_dir)

    PageSchema = get_page_schema()
    IssueSchema = get_issue_schema()
    pages = {}

    working_archive = os.path.join(issue_dir.path, "Document.zip")
    if os.path.isfile(working_archive):
        try:
            archive = zipfile.ZipFile(working_archive)
        except Exception as e:
            print("Bad zip file.")
            print("Unexpected error:", sys.exc_info()[0])
            print(out_dir + ";bad zip file;\n")
            return (issue_dir, False, e)

        # parse the XML files in the .zip archive
        counter = 0
        content_elements = []
        items = sorted(
            [
                item
                for item in archive.namelist()
                if ".xml" in item
                and not item.startswith("._")
                and ("/Ar" in item or "/Ad" in item)
            ]
        )
        # a dict mapping page numbers to names of page xml files
        page_xml_files = {
            int(item.split("/")[0]): item
            for item in archive.namelist()
            if ".xml" in item and not item.startswith("._") and "/Pg" in item
        }

        logger.debug("Contents: {}".format(archive.namelist()))

        # if a `temp_dir` is passed as a parameter, do store the intermediate
        # Olive .xml files, otherwise just read without storing them
        if temp_dir is not None:
            for item in items:
                with codecs.open(
                    os.path.join(temp_dir, item.replace('/', '-')),
                    'wb'
                ) as out_file:
                    xml_data = archive.read(item)
                    out_file.write(xml_data)

        # parse the `styleGallery.txt` file
        if 'styleGallery.txt' in archive.namelist():
            try:
                styles = parse_styles(
                    archive.read('styleGallery.txt').decode()
                )
            except Exception as e:
                logger.warning(
                    "Parsing style file in {} failed with error {}".format(
                        working_archive,
                        e
                    )
                )
                styles = []

        # parse the TOC
        toc_path = os.path.join(issue_dir.path, "TOC.xml")
        try:
            toc_data = olive_toc_parser(toc_path, issue_dir)
            logger.debug(toc_data)
        except FileNotFoundError as e:
            logger.error(f'Missing ToC.xml for {issue_dir.path}')
            logger.error(e)
            return (issue_dir, False, e)
        except Exception as e:
            logger.error(f'Corrupted ToC.xml for {issue_dir.path}')
            logger.error(e)
            return (issue_dir, False, e)

        logger.debug("XML files contained in {}: {}".format(
            working_archive,
            items
        ))
        articles = []
        all_element_ids = []

        # recompose each article by following the continuation links
        article_parts = []
        while len(items) > 0:
            counter += 1

            # if out file already exists skip the data it contains
            # TODO: change this to work with the JSON output
            """
            if os.path.exists(out_file):
                exclude_data = BeautifulSoup(open(out_file).read())
                exclude_data = [
                    x.meta.id.string
                    for x in exclude_data.find_all("entity")
                ]
                for y in exclude_data:
                    for z in items:
                        if y in z:
                            items.remove(z)
                continue
            """

            internal_deque = deque([items[0]])
            items = items[1:]

            while len(internal_deque) > 0:
                item = internal_deque.popleft()
                xml_data = archive.read(item).decode('windows-1252')
                new_data = olive_parser(xml_data)

                # check if it needs to be parsed later on
                if new_data["legacy"]['continuation_from'] is not None:
                    target = new_data["legacy"]["continuation_from"]
                    target = [x for x in items if target in x]
                    if len(target) > 0:
                        items.append(item)
                        continue

                article_parts.append(new_data)

                if new_data["legacy"]['continuation_to'] is not None:
                    next_id = new_data["legacy"]["continuation_to"]
                    next_id = [x for x in items if next_id in x][0]
                    internal_deque.append(next_id)
                    items.remove(next_id)

            try:
                content_elements += article_parts
                combined_article = combine_article_parts(article_parts)

                if combined_article is not None:
                    articles.append(combined_article)

                article_parts = []
            except Exception as e:
                """
                logger.error("Import of issue {} failed with error {}".format(
                    issue_dir,
                    e
                ))
                return
                """
                raise e

        # at this point the articles have been recomposed
        # but we still need to recompose pages
        for page_no in toc_data:
            logger.info(f'Processing page {page_no} of {issue_dir.path}')
            info_from_toc = toc_data[page_no]
            element_ids = toc_data[page_no].keys()

            # create a list of all element IDs for the issue.json
            all_element_ids = [
                el_id
                for el_id in toc_data[page_no].keys()
                if "Ar" in el_id or "Ad" in el_id
            ]

            elements = {
                el["legacy"]["id"]: el
                for el in content_elements
                if (el["legacy"]["id"] in element_ids)
            }
            page_dict = recompose_page(page_no, info_from_toc, elements)
            page = PageSchema(**page_dict)
            pages[page_no] = page
            # pages[page_no] = page_dict

        contents = recompose_ToC(toc_data, articles)
        pages_ids = [
            canonical_path(issue_dir, "p" + str(page_number).zfill(4))
            for page_number in sorted(pages.keys())
        ]
        issue_data = {
            "id": canonical_path(issue_dir, path_type="dir").replace("/", "-"),
            "s": styles,
            "i": contents,
            "pp": pages_ids
        }

        try:
            image_info = get_image_info(issue_dir, image_dir)
        except FileNotFoundError:
            logger.error(f"Missing image-info.json file for {issue_dir.path}")

        for page_no in pages:
            page = pages[page_no]

            # fail gracefully if no entry in image-info.json for this page
            try:
                image_info_record = [
                    page
                    for page in image_info
                    if int(page['pg']) == page_no
                ][0]
                box_strategy = image_info_record['strat']
                image_name = image_info_record['s']
                page_xml_file = page_xml_files[page_no]
            except Exception as e:
                logger.error("Page {} in {} raised error: {}".format(
                    page_no,
                    issue_data["id"],
                    e
                ))
                logger.error(
                    "Couldn't get information about page img {} in {}".format(
                        page_no,
                        issue_data["id"]
                    )
                )

            # convert the box coordinates
            try:
                convert_page_coordinates(
                    page,
                    archive.read(page_xml_file),
                    image_name,
                    archive,
                    box_strategy,
                    issue_dir
                )
                pages[page_no].cc = True
            except Exception as e:
                logger.error("Page {} in {} raised error: {}".format(
                    page_no,
                    issue_data["id"],
                    e
                ))
                logger.error(
                    "Couldn't convert coordinates in p. {} {}".format(
                        page_no,
                        issue_data["id"]
                    )
                )
                pages[page_no].cc = False

            # serialize the page object to JSON
            if out_dir is not None:
                serialize_page(
                    page_no,
                    page,
                    issue_dir,
                    out_dir=out_dir
                )
            elif s3_bucket is not None:
                serialize_page(
                    page_no,
                    page,
                    issue_dir,
                    s3_bucket=s3_bucket
                )

        issue = IssueSchema(**issue_data)
        if out_dir is not None:
            serialize_issue(issue, issue_dir, out_dir=out_dir)
        elif s3_bucket is not None:
            serialize_issue(issue, issue_dir, s3_bucket=s3_bucket)

        check_consistency(issue, all_element_ids)

        # 2. (TODO) check that each item in the issue.json
        # has at least one text region in the corresponding page file

    logger.info("Done importing '{}'".format(out_dir))
    return (issue_dir, True, None)
